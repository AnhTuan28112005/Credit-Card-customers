


%load_ext autoreload
%autoreload 2

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import os

sys.path.append(os.path.abspath('..'))
from src.data_processing import load_processed_data
from src.models import (
    LogisticRegression, KNN, accuracy_score, precision_score, 
    recall_score, f1_score, confusion_matrix, k_fold_cross_validation
)
sns.set_theme(style="whitegrid")





file_path = '../data/processed/bank_churn_processed.npz'
X_train, X_test, y_train, y_test = load_processed_data(file_path)

# Flatten y để tránh lỗi kích thước
y_train = y_train.flatten()
y_test = y_test.flatten()
print(f"Data Loaded: Train {X_train.shape}, Test {X_test.shape}")





print("--- TRAINING LOGISTIC REGRESSION (WITH CLASS WEIGHTS) ---")
# Learning rate nhỏ vừa phải, số vòng lặp lớn để hội tụ
model_lr = LogisticRegression(learning_rate=0.05, n_iterations=5000)
model_lr.fit(X_train, y_train)

# Vẽ biểu đồ Loss
plt.figure(figsize=(8, 4))
plt.plot(model_lr.losses)
plt.title('Loss Curve (Gradient Descent)')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.show()





print("--- THRESHOLD TUNING ---")
# 1. Lấy xác suất
y_probs = model_lr.predict_proba(X_test)

# 2. Tìm ngưỡng F1 cao nhất
thresholds = np.linspace(0, 1, 101)
f1_scores = [f1_score(y_test, np.array([1 if p > t else 0 for p in y_probs])) for t in thresholds]
precisions = [precision_score(y_test, np.array([1 if p > t else 0 for p in y_probs])) for t in thresholds]
recalls = [recall_score(y_test, np.array([1 if p > t else 0 for p in y_probs])) for t in thresholds]

best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx]

print(f"Best Threshold found: {best_threshold:.2f}")
print(f"Max F1-Score: {f1_scores[best_idx]:.4f}")

# 3. Vẽ biểu đồ Trade-off
plt.figure(figsize=(10, 5))
plt.plot(thresholds, precisions, label='Precision', linestyle='--')
plt.plot(thresholds, recalls, label='Recall', color='orange')
plt.plot(thresholds, f1_scores, label='F1 Score', color='green', linewidth=2)
plt.axvline(best_threshold, color='red', linestyle=':', label=f'Best {best_threshold:.2f}')
plt.title('Precision-Recall vs Threshold')
plt.legend()
plt.show()








print(f"--- FINAL EVALUATION (Threshold = {best_threshold:.2f}) ---")
y_pred_final = model_lr.predict(X_test, threshold=best_threshold)

print(f"Accuracy : {accuracy_score(y_test, y_pred_final):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_final):.4f}")
print(f"Recall   : {recall_score(y_test, y_pred_final):.4f}")
print(f"F1-Score : {f1_score(y_test, y_pred_final):.4f}")

# Confusion Matrix Visualization
tp, tn, fp, fn = confusion_matrix(y_test, y_pred_final)
cm = np.array([[tn, fp], [fn, tp]])
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Stay', 'Churn'], yticklabels=['Stay', 'Churn'])
plt.title('Confusion Matrix')
plt.show()








# Lấy trọng số và vẽ Top 10 quan trọng nhất
weights = model_lr.weights
sorted_idx = np.argsort(weights)
top_idx = np.concatenate((sorted_idx[:5], sorted_idx[-5:])) # 5 thấp nhất, 5 cao nhất

plt.figure(figsize=(10, 5))
colors = ['red' if w > 0 else 'green' for w in weights[top_idx]]
plt.barh(range(len(top_idx)), weights[top_idx], color=colors)
plt.title('Feature Importance (Weights Analysis)')
plt.xlabel('Weight Value (Positive=Churn Risk, Negative=Loyalty)')
plt.show()








print("--- COMPARISON WITH KNN ---")
knn = KNN(k=5)
knn.fit(X_train, y_train)

# Chạy trên tập nhỏ để nhanh (hoặc chạy full nếu máy mạnh)
y_pred_knn = knn.predict(X_test[:500])
f1_knn = f1_score(y_test[:500], y_pred_knn)

print(f"Logistic Regression F1: {f1_scores[best_idx]:.4f}")
print(f"KNN F1 (Subset)       : {f1_knn:.4f}")






